{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-pruning-main-example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBWQkwKYp5lg",
        "outputId": "be77ee06-8447-43d2-e7e6-0d11b987f5b9"
      },
      "source": [
        "!git clone https://github.com/qwes98/PyTorch-Pruning-Example"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch-Pruning-Example'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 49 (delta 23), reused 32 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRzWMJRrqTJO",
        "outputId": "73bef1e8-56b2-4ed3-ba9c-f1692c03c720"
      },
      "source": [
        "%cd PyTorch-Pruning-Example/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PyTorch-Pruning-Example\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfzhG-uBqZmE",
        "outputId": "95a993b1-d5b4-439d-a5a7-b95553e1b854"
      },
      "source": [
        "!python3 pretrain.py --epochs 50"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Command line arguments: Namespace(epochs=50, eval_batch_size=256, l1_regularization_strength=0, l2_regularization_strength=0.0001, model_dir='saved_models', model_filename='resnet18_cifar10.pt', n_cpu=8, train_batch_size=128)\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n",
            "170499072it [00:08, 19870680.85it/s]                   \n",
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Training Model...\n",
            "Epoch: 000 Eval Loss: 2.325 Eval Acc: 0.098\n",
            "Epoch: 001 Train Loss: 2.164 Train Acc: 0.263 Eval Loss: 1.714 Eval Acc: 0.355\n",
            "Epoch: 002 Train Loss: 1.649 Train Acc: 0.383 Eval Loss: 1.529 Eval Acc: 0.427\n",
            "Epoch: 003 Train Loss: 1.488 Train Acc: 0.452 Eval Loss: 1.377 Eval Acc: 0.490\n",
            "Epoch: 004 Train Loss: 1.329 Train Acc: 0.517 Eval Loss: 1.254 Eval Acc: 0.556\n",
            "Epoch: 005 Train Loss: 1.199 Train Acc: 0.570 Eval Loss: 1.110 Eval Acc: 0.600\n",
            "Epoch: 006 Train Loss: 1.088 Train Acc: 0.611 Eval Loss: 0.980 Eval Acc: 0.654\n",
            "Epoch: 007 Train Loss: 1.014 Train Acc: 0.642 Eval Loss: 1.011 Eval Acc: 0.643\n",
            "Epoch: 008 Train Loss: 0.951 Train Acc: 0.663 Eval Loss: 0.898 Eval Acc: 0.690\n",
            "Epoch: 009 Train Loss: 0.889 Train Acc: 0.686 Eval Loss: 0.850 Eval Acc: 0.702\n",
            "Epoch: 010 Train Loss: 0.846 Train Acc: 0.704 Eval Loss: 0.841 Eval Acc: 0.707\n",
            "Epoch: 011 Train Loss: 0.806 Train Acc: 0.719 Eval Loss: 0.779 Eval Acc: 0.735\n",
            "Epoch: 012 Train Loss: 0.776 Train Acc: 0.730 Eval Loss: 0.779 Eval Acc: 0.733\n",
            "Epoch: 013 Train Loss: 0.746 Train Acc: 0.739 Eval Loss: 0.781 Eval Acc: 0.730\n",
            "Epoch: 014 Train Loss: 0.720 Train Acc: 0.751 Eval Loss: 0.729 Eval Acc: 0.746\n",
            "Epoch: 015 Train Loss: 0.696 Train Acc: 0.758 Eval Loss: 0.715 Eval Acc: 0.751\n",
            "Epoch: 016 Train Loss: 0.673 Train Acc: 0.765 Eval Loss: 0.728 Eval Acc: 0.753\n",
            "Epoch: 017 Train Loss: 0.663 Train Acc: 0.768 Eval Loss: 0.666 Eval Acc: 0.772\n",
            "Epoch: 018 Train Loss: 0.647 Train Acc: 0.775 Eval Loss: 0.688 Eval Acc: 0.772\n",
            "Epoch: 019 Train Loss: 0.628 Train Acc: 0.780 Eval Loss: 0.733 Eval Acc: 0.748\n",
            "Epoch: 020 Train Loss: 0.618 Train Acc: 0.785 Eval Loss: 0.663 Eval Acc: 0.769\n",
            "Epoch: 021 Train Loss: 0.605 Train Acc: 0.791 Eval Loss: 0.660 Eval Acc: 0.776\n",
            "Epoch: 022 Train Loss: 0.592 Train Acc: 0.791 Eval Loss: 0.674 Eval Acc: 0.768\n",
            "Epoch: 023 Train Loss: 0.581 Train Acc: 0.799 Eval Loss: 0.663 Eval Acc: 0.779\n",
            "Epoch: 024 Train Loss: 0.574 Train Acc: 0.801 Eval Loss: 0.655 Eval Acc: 0.777\n",
            "Epoch: 025 Train Loss: 0.555 Train Acc: 0.808 Eval Loss: 0.681 Eval Acc: 0.771\n",
            "Epoch: 026 Train Loss: 0.551 Train Acc: 0.809 Eval Loss: 0.629 Eval Acc: 0.788\n",
            "Epoch: 027 Train Loss: 0.546 Train Acc: 0.811 Eval Loss: 0.635 Eval Acc: 0.784\n",
            "Epoch: 028 Train Loss: 0.533 Train Acc: 0.815 Eval Loss: 0.627 Eval Acc: 0.791\n",
            "Epoch: 029 Train Loss: 0.526 Train Acc: 0.818 Eval Loss: 0.640 Eval Acc: 0.787\n",
            "Epoch: 030 Train Loss: 0.521 Train Acc: 0.819 Eval Loss: 0.650 Eval Acc: 0.785\n",
            "Epoch: 031 Train Loss: 0.517 Train Acc: 0.821 Eval Loss: 0.645 Eval Acc: 0.785\n",
            "Epoch: 032 Train Loss: 0.505 Train Acc: 0.823 Eval Loss: 0.599 Eval Acc: 0.796\n",
            "Epoch: 033 Train Loss: 0.501 Train Acc: 0.827 Eval Loss: 0.620 Eval Acc: 0.798\n",
            "Epoch: 034 Train Loss: 0.489 Train Acc: 0.829 Eval Loss: 0.612 Eval Acc: 0.796\n",
            "Epoch: 035 Train Loss: 0.494 Train Acc: 0.828 Eval Loss: 0.566 Eval Acc: 0.808\n",
            "Epoch: 036 Train Loss: 0.478 Train Acc: 0.833 Eval Loss: 0.584 Eval Acc: 0.802\n",
            "Epoch: 037 Train Loss: 0.475 Train Acc: 0.833 Eval Loss: 0.585 Eval Acc: 0.803\n",
            "Epoch: 038 Train Loss: 0.471 Train Acc: 0.835 Eval Loss: 0.556 Eval Acc: 0.814\n",
            "Epoch: 039 Train Loss: 0.468 Train Acc: 0.839 Eval Loss: 0.589 Eval Acc: 0.806\n",
            "Epoch: 040 Train Loss: 0.463 Train Acc: 0.840 Eval Loss: 0.592 Eval Acc: 0.807\n",
            "Epoch: 041 Train Loss: 0.457 Train Acc: 0.841 Eval Loss: 0.596 Eval Acc: 0.802\n",
            "Epoch: 042 Train Loss: 0.457 Train Acc: 0.841 Eval Loss: 0.619 Eval Acc: 0.795\n",
            "Epoch: 043 Train Loss: 0.450 Train Acc: 0.842 Eval Loss: 0.548 Eval Acc: 0.816\n",
            "Epoch: 044 Train Loss: 0.449 Train Acc: 0.843 Eval Loss: 0.616 Eval Acc: 0.796\n",
            "Epoch: 045 Train Loss: 0.445 Train Acc: 0.845 Eval Loss: 0.604 Eval Acc: 0.802\n",
            "Epoch: 046 Train Loss: 0.437 Train Acc: 0.850 Eval Loss: 0.564 Eval Acc: 0.810\n",
            "Epoch: 047 Train Loss: 0.434 Train Acc: 0.849 Eval Loss: 0.546 Eval Acc: 0.817\n",
            "Epoch: 048 Train Loss: 0.430 Train Acc: 0.851 Eval Loss: 0.557 Eval Acc: 0.817\n",
            "Epoch: 049 Train Loss: 0.431 Train Acc: 0.850 Eval Loss: 0.603 Eval Acc: 0.801\n",
            "Epoch: 050 Train Loss: 0.423 Train Acc: 0.854 Eval Loss: 0.573 Eval Acc: 0.810\n",
            "Test Accuracy: 0.810\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.88      0.83      1000\n",
            "           1       0.89      0.90      0.90      1000\n",
            "           2       0.72      0.80      0.76      1000\n",
            "           3       0.68      0.64      0.66      1000\n",
            "           4       0.82      0.76      0.79      1000\n",
            "           5       0.71      0.74      0.73      1000\n",
            "           6       0.81      0.89      0.85      1000\n",
            "           7       0.93      0.73      0.82      1000\n",
            "           8       0.89      0.89      0.89      1000\n",
            "           9       0.91      0.86      0.88      1000\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.81      0.81      0.81     10000\n",
            "weighted avg       0.81      0.81      0.81     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epqhTwaxqtpF",
        "outputId": "194ee7e8-0d4b-43c4-f569-cdc70c70ea77"
      },
      "source": [
        "!python3 prune.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Test Accuracy: 0.785\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.89      0.78      1000\n",
            "           1       0.92      0.82      0.87      1000\n",
            "           2       0.72      0.76      0.74      1000\n",
            "           3       0.77      0.47      0.58      1000\n",
            "           4       0.78      0.78      0.78      1000\n",
            "           5       0.69      0.71      0.70      1000\n",
            "           6       0.80      0.88      0.84      1000\n",
            "           7       0.81      0.84      0.83      1000\n",
            "           8       0.88      0.85      0.87      1000\n",
            "           9       0.84      0.85      0.84      1000\n",
            "\n",
            "    accuracy                           0.78     10000\n",
            "   macro avg       0.79      0.78      0.78     10000\n",
            "weighted avg       0.79      0.78      0.78     10000\n",
            "\n",
            "Global Sparsity:\n",
            "0.00\n",
            "Iterative Pruning + Fine-Tuning...\n",
            "Pruning and Finetuning 1/5\n",
            "Pruning...\n",
            "Test Accuracy: 0.612\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.92      0.58      1000\n",
            "           1       0.98      0.37      0.53      1000\n",
            "           2       0.70      0.61      0.65      1000\n",
            "           3       0.52      0.49      0.51      1000\n",
            "           4       0.51      0.89      0.65      1000\n",
            "           5       0.90      0.23      0.36      1000\n",
            "           6       0.62      0.81      0.70      1000\n",
            "           7       0.76      0.76      0.76      1000\n",
            "           8       0.93      0.31      0.47      1000\n",
            "           9       0.72      0.74      0.73      1000\n",
            "\n",
            "    accuracy                           0.61     10000\n",
            "   macro avg       0.71      0.61      0.59     10000\n",
            "weighted avg       0.71      0.61      0.59     10000\n",
            "\n",
            "Global Sparsity:\n",
            "0.98\n",
            "Fine-tuning...\n",
            "Epoch: 000 Eval Loss: 1.128 Eval Acc: 0.612\n",
            "Epoch: 001 Train Loss: 0.583 Train Acc: 0.802 Eval Loss: 0.612 Eval Acc: 0.795\n",
            "Epoch: 002 Train Loss: 0.528 Train Acc: 0.819 Eval Loss: 0.590 Eval Acc: 0.806\n",
            "Epoch: 003 Train Loss: 0.504 Train Acc: 0.825 Eval Loss: 0.574 Eval Acc: 0.811\n",
            "Epoch: 004 Train Loss: 0.486 Train Acc: 0.831 Eval Loss: 0.569 Eval Acc: 0.814\n",
            "Epoch: 005 Train Loss: 0.476 Train Acc: 0.836 Eval Loss: 0.561 Eval Acc: 0.814\n",
            "Epoch: 006 Train Loss: 0.468 Train Acc: 0.840 Eval Loss: 0.551 Eval Acc: 0.817\n",
            "Epoch: 007 Train Loss: 0.458 Train Acc: 0.842 Eval Loss: 0.549 Eval Acc: 0.818\n",
            "Epoch: 008 Train Loss: 0.451 Train Acc: 0.844 Eval Loss: 0.549 Eval Acc: 0.817\n",
            "Epoch: 009 Train Loss: 0.448 Train Acc: 0.844 Eval Loss: 0.546 Eval Acc: 0.822\n",
            "Epoch: 010 Train Loss: 0.444 Train Acc: 0.846 Eval Loss: 0.539 Eval Acc: 0.824\n",
            "Test Accuracy: 0.824\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84      1000\n",
            "           1       0.90      0.91      0.90      1000\n",
            "           2       0.77      0.76      0.77      1000\n",
            "           3       0.70      0.66      0.68      1000\n",
            "           4       0.79      0.82      0.80      1000\n",
            "           5       0.73      0.70      0.72      1000\n",
            "           6       0.86      0.90      0.88      1000\n",
            "           7       0.88      0.86      0.87      1000\n",
            "           8       0.92      0.89      0.90      1000\n",
            "           9       0.87      0.88      0.88      1000\n",
            "\n",
            "    accuracy                           0.82     10000\n",
            "   macro avg       0.82      0.82      0.82     10000\n",
            "weighted avg       0.82      0.82      0.82     10000\n",
            "\n",
            "Global Sparsity:\n",
            "0.98\n",
            "Pruning and Finetuning 2/5\n",
            "Pruning...\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Test Accuracy: 0.100\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1000\n",
            "           1       0.00      0.00      0.00      1000\n",
            "           2       0.00      0.00      0.00      1000\n",
            "           3       0.00      0.00      0.00      1000\n",
            "           4       0.00      0.00      0.00      1000\n",
            "           5       0.00      0.00      0.00      1000\n",
            "           6       0.00      0.00      0.00      1000\n",
            "           7       0.00      0.00      0.00      1000\n",
            "           8       0.00      0.00      0.00      1000\n",
            "           9       0.10      1.00      0.18      1000\n",
            "\n",
            "    accuracy                           0.10     10000\n",
            "   macro avg       0.01      0.10      0.02     10000\n",
            "weighted avg       0.01      0.10      0.02     10000\n",
            "\n",
            "Global Sparsity:\n",
            "1.00\n",
            "Fine-tuning...\n",
            "Epoch: 000 Eval Loss: 2.842 Eval Acc: 0.100\n",
            "Epoch: 001 Train Loss: 2.494 Train Acc: 0.142 Eval Loss: 2.221 Eval Acc: 0.162\n",
            "Epoch: 002 Train Loss: 2.186 Train Acc: 0.184 Eval Loss: 2.112 Eval Acc: 0.203\n",
            "Epoch: 003 Train Loss: 2.101 Train Acc: 0.211 Eval Loss: 2.051 Eval Acc: 0.225\n",
            "Epoch: 004 Train Loss: 2.057 Train Acc: 0.227 Eval Loss: 2.002 Eval Acc: 0.250\n",
            "Epoch: 005 Train Loss: 2.017 Train Acc: 0.242 Eval Loss: 1.961 Eval Acc: 0.271\n",
            "Epoch: 006 Train Loss: 1.986 Train Acc: 0.254 Eval Loss: 1.927 Eval Acc: 0.285\n",
            "Traceback (most recent call last):\n",
            "  File \"prune.py\", line 329, in <module>\n",
            "    main()\n",
            "  File \"prune.py\", line 303, in main\n",
            "    grouped_pruning=True)\n",
            "  File \"prune.py\", line 159, in iterative_pruning_finetuning\n",
            "    num_epochs=num_epochs_per_iteration)\n",
            "  File \"/content/PyTorch-Pruning-Example/utils.py\", line 169, in train_model\n",
            "    for inputs, labels in train_loader:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 517, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1182, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1148, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 986, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 91, in get\n",
            "    def get(self, block=True, timeout=None):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDn1hK73yBd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}